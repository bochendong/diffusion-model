{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"\n",
    "    Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\n",
    "    \"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "files = glob.glob(\"./output/*.png\")\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epoches = 200\n",
    "ema_decay = 0.999\n",
    "steps = 500\n",
    "eta = 1.\n",
    "\n",
    "guidance_scale = 2.\n",
    "\n",
    "def load_data_set(batch_size=64):\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "\n",
    "    # Load the entire CIFAR10 dataset\n",
    "    full_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf)\n",
    "\n",
    "    # Create filtered Subsets for each category\n",
    "    source_indices = [i for i, (_, label) in enumerate(full_dataset) if label <= 9]\n",
    "\n",
    "\n",
    "    source_set = Subset(full_dataset, source_indices)\n",
    "\n",
    "    # Create DataLoaders for each subset\n",
    "    source_dl = DataLoader(source_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    return source_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input) + self.skip(input)\n",
    "\n",
    "class ResConvBlock(ResidualBlock):\n",
    "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
    "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        super().__init__([\n",
    "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
    "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
    "        ], skip)\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert c_in % n_head == 0\n",
    "        self.norm = nn.GroupNorm(1, c_in)\n",
    "        self.n_head = n_head\n",
    "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
    "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "        qkv = self.qkv_proj(self.norm(input))\n",
    "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = k.shape[3]**-0.25\n",
    "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
    "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
    "        return input + self.dropout(self.out_proj(y))\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Linear(1000, 3072),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        self.pretrained_model = self.pretrained_model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.pretrained_model(x)\n",
    "        x = self.fe(x)\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        return x\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 2),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 256 * 8 * 8)\n",
    "        return self.net(input)\n",
    "\n",
    "def expand_to_planes(input, shape):\n",
    "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffussion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.avg_1 = nn.AvgPool2d(2)\n",
    "        self.avg_2 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.down_0 = nn.Sequential(\n",
    "            ResConvBlock(3 + 16 + 4, c, c),\n",
    "            ResConvBlock(c, c, c),\n",
    "        )\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            ResConvBlock(c, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c * 2),\n",
    "        )\n",
    "\n",
    "        self.down_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        down_sample = self.down_0(input)\n",
    "        identity_0 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_1(self.down_1(down_sample))\n",
    "        identity_1 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_2(self.down_2(down_sample))\n",
    "        identity_2 = nn.Identity()(down_sample)\n",
    "\n",
    "        return down_sample, identity_0, identity_1, identity_2\n",
    "\n",
    "class MidSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.avg_3 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.mid_1 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "        self.mid_2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, down_sample):\n",
    "        middle_sample = self.mid_1(down_sample)\n",
    "        middle_sample = self.avg_3(self.mid_2(middle_sample))\n",
    "\n",
    "        return middle_sample\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.up_1 = nn.Sequential(\n",
    "            ResConvBlock(c * 8, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 2),\n",
    "            SelfAttention2d(c * 2, c * 2 // 64),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_3 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c, c),\n",
    "            ResConvBlock(c, c, 3, is_last=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, middle_sample, identity_0, identity_1, identity_2):\n",
    "        up_sample = torch.cat([middle_sample, identity_2], dim=1)\n",
    "        up_sample = self.up_1(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_1], dim=1)\n",
    "        up_sample = self.up_2(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_0], dim=1)\n",
    "\n",
    "        return self.up_3(up_sample)\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.feature_embed = FeatureEmbedding()\n",
    "        self.domain_classifier = DomainClassifier()\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.class_embed = nn.Embedding(11, 4)\n",
    "\n",
    "        self.down_sample_net = DownSample()\n",
    "        self.mid_sample_net = MidSample()\n",
    "        self.up_sample_net = UpSample()\n",
    "\n",
    "    def forward(self, input, t, cond):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
    "        x = torch.cat([input, timestep_embed, class_embed], dim=1)\n",
    "\n",
    "        down_sample, identity_0, identity_1, identity_2 = self.down_sample_net(x)\n",
    "        middle_sample = self.mid_sample_net(down_sample)\n",
    "        up_sample = self.up_sample_net(middle_sample, identity_0, identity_1, identity_2)\n",
    "\n",
    "        return up_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
    "    \"\"\"\n",
    "    Draws samples from a model given starting noise.\n",
    "    \"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in range(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x_in = torch.cat([x, x])\n",
    "            ts_in = torch.cat([ts, ts])\n",
    "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
    "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in).float().chunk(2)\n",
    "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "steps = 500\n",
    "eta = 1.\n",
    "ema_decay = 0.999\n",
    "guidance_scale = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffussion_target(images, labels):\n",
    "    t = rng.draw(labels.shape[0])[:, 0].to(device)\n",
    "\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    alphas = alphas[:, None, None, None]\n",
    "    sigmas = sigmas[:, None, None, None]\n",
    "\n",
    "    noise = torch.randn_like(images)\n",
    "    noised_reals = images * alphas + noise * sigmas\n",
    "\n",
    "    targets = noise * alphas - images * sigmas\n",
    "\n",
    "    return t, noised_reals, targets\n",
    "\n",
    "def train_diffussion(epoch, model, train_dl, optimizer, scaler):\n",
    "    model.train()\n",
    "    for src_images, src_labels in train_dl:\n",
    "        src_images, src_labels = src_images.to(device), src_labels.to(device)\n",
    "\n",
    "        t, noised_src, src_recon_targets = generate_diffussion_target(src_images, src_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        to_drop = torch.rand(src_labels.shape, device=src_labels.device).le(0.2)\n",
    "        classes_drop = torch.where(to_drop, -torch.ones_like(src_labels), src_labels)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(noised_src, t, classes_drop)\n",
    "            diffused_loss = F.mse_loss(output, src_recon_targets)\n",
    "        \n",
    "        loss = diffused_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print('diff', loss.item())\n",
    "    noise = torch.randn([10, 3, 32, 32], device=device)\n",
    "    fakes_classes = torch.arange(10, device=device)\n",
    "    fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
    "    fakes = (fakes + 1) / 2\n",
    "    fakes = torch.clamp(fakes, min=0, max = 1)\n",
    "    save_image(fakes.data, './output/%03d_train.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dl = load_data_set(batch_size = batch_size)\n",
    "criterion = nn.NLLLoss()\n",
    "model = Diffusion().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradScaler' object has no attribute 'param_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mtrain_diffussion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_diffussion\u001b[1;34m(epoch, model, train_dl, optimizer, scaler)\u001b[0m\n\u001b[0;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m diffused_loss\n\u001b[0;32m     33\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:334\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    276\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    277\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 279\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:202\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    200\u001b[0m per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))  \u001b[38;5;66;03m# type: ignore[var-annotated]\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GradScaler' object has no attribute 'param_groups'"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "active_domian_loss = 50\n",
    "src_domain_label = torch.zeros(batch_size).long().to(device)\n",
    "tgt_domain_label = torch.ones(batch_size).long().to(device)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        train_diffussion(epoch, model, train_dl, optimizer, scaler)\n",
    "        epoch += 1\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/09_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/020_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/029_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/039_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/049_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
