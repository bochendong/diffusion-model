{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhlDCo9PcAqfsRRRupoZoK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94ad67bd73814c13ac24d9f239f1f145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50bd88a9ae654fe292b08f400e6d1f22",
              "IPY_MODEL_b326bb577b024702be6e2b16c342d71a",
              "IPY_MODEL_a0de68306d5648dc9596e97ca5a83152"
            ],
            "layout": "IPY_MODEL_6ed89076d05d41ecaf0b42e01f4eed19"
          }
        },
        "50bd88a9ae654fe292b08f400e6d1f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_859efea98d3f4c6d8b07e64319afaaf7",
            "placeholder": "​",
            "style": "IPY_MODEL_e3847efff8874fc592dc32312257c5ed",
            "value": " 20%"
          }
        },
        "b326bb577b024702be6e2b16c342d71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc601df7e81744509d433c25111cf333",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bef72b25bd744827ae397f5f18777bb7",
            "value": 9977
          }
        },
        "a0de68306d5648dc9596e97ca5a83152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b9e551fda4e45d19e5bcadefe00cd3e",
            "placeholder": "​",
            "style": "IPY_MODEL_9958edc7e4944f828bec9426d1e36060",
            "value": " 9977/50000 [01:28&lt;04:57, 134.53it/s]"
          }
        },
        "6ed89076d05d41ecaf0b42e01f4eed19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859efea98d3f4c6d8b07e64319afaaf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3847efff8874fc592dc32312257c5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc601df7e81744509d433c25111cf333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef72b25bd744827ae397f5f18777bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b9e551fda4e45d19e5bcadefe00cd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9958edc7e4944f828bec9426d1e36060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/diffusion-model/blob/main/DDPM/DDPM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1lu-xkj_DbM"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9q4lPoJf0-ly"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_alphas_sigmas(t):\n",
        "    \"\"\"\n",
        "    Returns the scaling factors for the clean image (alpha) and for the\n",
        "    noise (sigma), given a timestep.\n",
        "    \"\"\"\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "files = glob.glob(\"./output/*.png\")\n",
        "\n",
        "for f in files:\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "bLGurZOw_KRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Set"
      ],
      "metadata": {
        "id": "SwW2cWHzugrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epoches = 30\n",
        "ema_decay = 0.999\n",
        "steps = 500\n",
        "eta = 1.\n",
        "\n",
        "guidance_scale = 2.\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(root='data', train=True, download=True, transform=tf, target_transform=lambda x: x if x < 5 else -1)\n",
        "train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuyTXC4x_O0C",
        "outputId": "73e39a27-e79f-436c-eabf-ba33b256640e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = resnet50(pretrained=True)\n",
        "pretrained_model.fc = nn.Identity()\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "pretrained_model.eval()"
      ],
      "metadata": {
        "id": "Bmefgb7cCroq",
        "outputId": "7847f97d-1bba-4e1e-eab9-183e7ed65be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "def extract_and_load_features(dataset, feature_extractor, device):\n",
        "    feature_extractor.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    for image, label in tqdm(dataset):\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            extracted_feature = feature_extractor(image).squeeze(0)\n",
        "        features.append(extracted_feature.cpu())\n",
        "        labels.append(label)\n",
        "    features_tensor = torch.stack(features)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    return DataLoader(TensorDataset(features_tensor, labels_tensor), batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "stage2_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf, target_transform=lambda x: x if 5 <= x <= 7 else -1)\n",
        "feature_dl = extract_and_load_features(stage2_dataset, pretrained_model, device)"
      ],
      "metadata": {
        "id": "EanghDU5u557",
        "outputId": "25e99698-87f9-486f-f9ae-2b00dd8d57ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "94ad67bd73814c13ac24d9f239f1f145",
            "50bd88a9ae654fe292b08f400e6d1f22",
            "b326bb577b024702be6e2b16c342d71a",
            "a0de68306d5648dc9596e97ca5a83152",
            "6ed89076d05d41ecaf0b42e01f4eed19",
            "859efea98d3f4c6d8b07e64319afaaf7",
            "e3847efff8874fc592dc32312257c5ed",
            "cc601df7e81744509d433c25111cf333",
            "bef72b25bd744827ae397f5f18777bb7",
            "9b9e551fda4e45d19e5bcadefe00cd3e",
            "9958edc7e4944f828bec9426d1e36060"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94ad67bd73814c13ac24d9f239f1f145"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "QUGf43CtukEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model (a residual U-Net)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input) + self.skip(input)\n",
        "\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
        "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
        "        ], skip)\n",
        "\n",
        "\n",
        "class SelfAttention2d(nn.Module):\n",
        "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        assert c_in % n_head == 0\n",
        "        self.norm = nn.GroupNorm(1, c_in)\n",
        "        self.n_head = n_head\n",
        "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
        "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        qkv = self.qkv_proj(self.norm(input))\n",
        "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = k.shape[3]**-0.25\n",
        "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
        "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
        "        return input + self.dropout(self.out_proj(y))\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "\n",
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.class_embed = nn.Embedding(11, 4)\n",
        "\n",
        "        self.net = nn.Sequential(   # 32x32\n",
        "            ResConvBlock(3 + 16 + 4, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            SkipBlock([\n",
        "                nn.AvgPool2d(2),  # 32x32 -> 16x16\n",
        "                ResConvBlock(c, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.AvgPool2d(2),  # 16x16 -> 8x8\n",
        "                    ResConvBlock(c * 2, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    SkipBlock([\n",
        "                        nn.AvgPool2d(2),  # 8x8 -> 4x4\n",
        "                        ResConvBlock(c * 4, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 4),\n",
        "                        SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                    ]),  # 4x4 -> 8x8\n",
        "                    ResConvBlock(c * 8, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 2),\n",
        "                    SelfAttention2d(c * 2, c * 2 // 64),\n",
        "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                ]),  # 8x8 -> 16x16\n",
        "                ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ]),  # 16x16 -> 32x32\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, is_last=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t, cond):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
        "        return self.net(torch.cat([input, class_embed, timestep_embed], dim=1))"
      ],
      "metadata": {
        "id": "2ABRAZs-_EhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
        "    \"\"\"\n",
        "    Draws samples from a model given starting noise.\n",
        "    \"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
        "    alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(steps):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x_in = torch.cat([x, x])\n",
        "            ts_in = torch.cat([ts, ts])\n",
        "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
        "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in).float().chunk(2)\n",
        "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < steps - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred"
      ],
      "metadata": {
        "id": "o7Tm8u5vJhJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureIntegration(nn.Module):\n",
        "    def __init__(self, feature_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(feature_dim, output_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.activation(self.fc1(features))\n",
        "\n",
        "# Assuming the pre-trained feature dimension is 2048, and we want to integrate these into a 64x8x8 feature map\n",
        "feature_integration_output_dim = 64 * 8 * 8  # Adjust as needed to fit your model's architecture\n",
        "\n",
        "class DiffusionWithFeatures(Diffusion):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        feature_integration_output_dim = 64 * 32 * 32  # Assuming we need to reshape to 32 x 32\n",
        "        self.feature_integration = FeatureIntegration(2048, feature_integration_output_dim)\n",
        "        # Adding a layer to adjust channels after concatenation\n",
        "        self.adjust_channels = nn.Conv2d(87, 23, kernel_size=1)  # Adjusting to the expected channels\n",
        "\n",
        "    def forward(self, input, t, cond, external_features=None):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
        "        combined_input = torch.cat([input, class_embed, timestep_embed], dim=1)\n",
        "\n",
        "        if external_features is not None:\n",
        "            external_features = self.feature_integration(external_features)\n",
        "            external_features = external_features.view(-1, 64, 32, 32)  # Reshape to match combined_input\n",
        "            combined_input = torch.cat([combined_input, external_features], dim=1)\n",
        "            combined_input = self.adjust_channels(combined_input)  # Adjust channels\n",
        "\n",
        "        return self.net(combined_input)\n"
      ],
      "metadata": {
        "id": "uXMRwa5Te6NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Stage"
      ],
      "metadata": {
        "id": "uIwAe5GJupC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)"
      ],
      "metadata": {
        "id": "SxYYFSglmJX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, train_dl, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in tqdm(train_dl):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Generate random timesteps\n",
        "            t = rng.draw(labels.shape[0])[:, 0].to(device)\n",
        "\n",
        "            # Get alphas and sigmas\n",
        "            alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "            # Apply noise model\n",
        "            alphas = alphas[:, None, None, None]\n",
        "            sigmas = sigmas[:, None, None, None]\n",
        "\n",
        "            noise = torch.randn_like(images)\n",
        "            noised_reals = images * alphas + noise * sigmas\n",
        "\n",
        "            # with t increase, alpha goes down and sigmas goes up\n",
        "            # which means with t increase, targets will closer to - img\n",
        "            targets = noise * alphas - images * sigmas\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(noised_reals, t, labels)\n",
        "                loss = F.mse_loss(output, targets)  # Example loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
        "        noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "        fakes_classes = torch.arange(10, device=device)\n",
        "\n",
        "        fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "        fakes = (fakes + 1) / 2\n",
        "        fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "        save_image(fakes.data, './output/%03d_train.png' % epoch)"
      ],
      "metadata": {
        "id": "ytcz0Jxgjrnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_features(model, optimizer, train_dl, feature_dl, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, ((reals, classes), (features, feature_labels)) in enumerate(zip(train_dl, feature_dl)):\n",
        "            optimizer.zero_grad()\n",
        "            reals, classes = reals.to(device), classes.to(device)\n",
        "            features = features.to(device)\n",
        "\n",
        "            t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "            alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "            alphas = alphas[:, None, None, None]\n",
        "            sigmas = sigmas[:, None, None, None]\n",
        "\n",
        "            noise = torch.randn_like(reals)\n",
        "            noised_reals = reals * alphas + noise * sigmas\n",
        "            targets = noise * alphas - reals * sigmas\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                v = model(noised_reals, t, classes, external_features=features)\n",
        "                loss = F.mse_loss(v, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "        fakes_classes = torch.arange(10, device=device)\n",
        "\n",
        "        fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "        fakes = (fakes + 1) / 2\n",
        "        fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "        save_image(fakes.data, './output/%03d_imagine.png' % epoch)"
      ],
      "metadata": {
        "id": "ifO4jcg7e8WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "D7r2NUnavAKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionWithFeatures().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "XtAefZ2FjiS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, optimizer, train_dl, epochs=epoches, device=device)"
      ],
      "metadata": {
        "id": "bSYF-8DekXGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(\"./output/010_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6BpX1b2KwtMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/020_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ohsXa-IgwzJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/030_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q6mSPm2GxFGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with features\n",
        "train_with_features(model, optimizer, train_dl, feature_dl, epochs=epoches, device=device)"
      ],
      "metadata": {
        "id": "O9nLL3abkdTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/010_imagine.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VaTuXj0FxHb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/020_imagine.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xbw8o3CPxJAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/030_imagine.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dz0C3c5WxJFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def train(epoch):\n",
        "    for i, (reals, classes) in enumerate(tqdm(train_dl)):\n",
        "        opt.zero_grad()\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        # Draw uniformly distributed continuous timesteps\n",
        "        t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "\n",
        "        # Calculate the noise schedule parameters for those timesteps\n",
        "        alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "        # Combine the ground truth images and the noise\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        noise = torch.randn_like(reals)\n",
        "\n",
        "        # with t increase, alpha goes down and sigmas goes up\n",
        "        # which means with t increase, noised_reals will have more noise and less image\n",
        "        noised_reals = reals * alphas + noise * sigmas\n",
        "\n",
        "        # with t increase, alpha goes down and sigmas goes up\n",
        "        # which means with t increase, targets will closer to - img\n",
        "        targets = noise * alphas - reals * sigmas\n",
        "\n",
        "        # Drop out the class on 20% of the examples\n",
        "        to_drop = torch.rand(classes.shape, device=classes.device).le(0.2)\n",
        "        classes_drop = torch.where(to_drop, -torch.ones_like(classes), classes)\n",
        "\n",
        "        # Compute the model output and the loss.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            v = model(noised_reals, t, classes_drop)\n",
        "            loss = F.mse_loss(v, targets)\n",
        "\n",
        "        # Do the optimizer step and EMA update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "\n",
        "\n",
        "        # ema_update(model, model_ema, 0.95 if epoch < 20 else ema_decay)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            tqdm.write(f'Epoch: {epoch}, iteration: {i}, loss: {loss.item():g}')\n",
        "\n",
        "    noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "    fakes_classes = torch.arange(10, device=device)\n",
        "\n",
        "    fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "    fakes = (fakes + 1) / 2\n",
        "    fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "    save_image(fakes.data, './output/%03d.png' % epoch)\n",
        "'''"
      ],
      "metadata": {
        "id": "5Hl1eaHr_d4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''try:\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        train(i)\n",
        "except KeyboardInterrupt:\n",
        "    pass'''"
      ],
      "metadata": {
        "id": "uV9DM-JvAFYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}