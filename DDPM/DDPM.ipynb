{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"\n",
    "    Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\n",
    "    \"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "files = glob.glob(\"./output/*.png\")\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epoches = 200\n",
    "ema_decay = 0.999\n",
    "steps = 500\n",
    "eta = 1.\n",
    "\n",
    "guidance_scale = 2.\n",
    "\n",
    "def load_data_set(batch_size=64):\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "\n",
    "    # Load the entire CIFAR10 dataset\n",
    "    full_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf)\n",
    "\n",
    "    # Create filtered Subsets for each category\n",
    "    source_indices = [i for i, (_, label) in enumerate(full_dataset) if label <= 9]\n",
    "\n",
    "\n",
    "    source_set = Subset(full_dataset, source_indices)\n",
    "\n",
    "    # Create DataLoaders for each subset\n",
    "    source_dl = DataLoader(source_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    return source_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input) + self.skip(input)\n",
    "\n",
    "class ResConvBlock(ResidualBlock):\n",
    "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
    "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        super().__init__([\n",
    "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
    "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
    "        ], skip)\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert c_in % n_head == 0\n",
    "        self.norm = nn.GroupNorm(1, c_in)\n",
    "        self.n_head = n_head\n",
    "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
    "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "        qkv = self.qkv_proj(self.norm(input))\n",
    "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = k.shape[3]**-0.25\n",
    "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
    "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
    "        return input + self.dropout(self.out_proj(y))\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Linear(1000, 3072),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        self.pretrained_model = self.pretrained_model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.pretrained_model(x)\n",
    "        x = self.fe(x)\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        return x\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 2),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 256 * 8 * 8)\n",
    "        return self.net(input)\n",
    "\n",
    "def expand_to_planes(input, shape):\n",
    "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffussion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.avg_1 = nn.AvgPool2d(2)\n",
    "        self.avg_2 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.down_0 = nn.Sequential(\n",
    "            ResConvBlock(3 + 16 + 4, c, c),\n",
    "            ResConvBlock(c, c, c),\n",
    "        )\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            ResConvBlock(c, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c * 2),\n",
    "        )\n",
    "\n",
    "        self.down_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        down_sample = self.down_0(input)\n",
    "        identity_0 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_1(self.down_1(down_sample))\n",
    "        identity_1 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_2(self.down_2(down_sample))\n",
    "        identity_2 = nn.Identity()(down_sample)\n",
    "\n",
    "        return down_sample, identity_0, identity_1, identity_2\n",
    "\n",
    "class MidSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.avg_3 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.mid_1 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "        self.mid_2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, down_sample):\n",
    "        middle_sample = self.mid_1(down_sample)\n",
    "        middle_sample = self.avg_3(self.mid_2(middle_sample))\n",
    "\n",
    "        return middle_sample\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.up_1 = nn.Sequential(\n",
    "            ResConvBlock(c * 8, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 2),\n",
    "            SelfAttention2d(c * 2, c * 2 // 64),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_3 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c, c),\n",
    "            ResConvBlock(c, c, 3, is_last=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, middle_sample, identity_0, identity_1, identity_2):\n",
    "        up_sample = torch.cat([middle_sample, identity_2], dim=1)\n",
    "        up_sample = self.up_1(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_1], dim=1)\n",
    "        up_sample = self.up_2(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_0], dim=1)\n",
    "\n",
    "        return self.up_3(up_sample)\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.feature_embed = FeatureEmbedding()\n",
    "        self.domain_classifier = DomainClassifier()\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.class_embed = nn.Embedding(11, 4)\n",
    "\n",
    "        self.down_sample_net = DownSample()\n",
    "        self.mid_sample_net = MidSample()\n",
    "        self.up_sample_net = UpSample()\n",
    "\n",
    "    def forward(self, input, t, cond):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
    "        x = torch.cat([input, timestep_embed, class_embed], dim=1)\n",
    "\n",
    "        down_sample, identity_0, identity_1, identity_2 = self.down_sample_net(x)\n",
    "        middle_sample = self.mid_sample_net(down_sample)\n",
    "        up_sample = self.up_sample_net(middle_sample, identity_0, identity_1, identity_2)\n",
    "\n",
    "        return up_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
    "    \"\"\"\n",
    "    Draws samples from a model given starting noise.\n",
    "    \"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in range(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x_in = torch.cat([x, x])\n",
    "            ts_in = torch.cat([ts, ts])\n",
    "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
    "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in)[0].float().chunk(2)\n",
    "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "steps = 500\n",
    "eta = 1.\n",
    "ema_decay = 0.999\n",
    "guidance_scale = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffussion_target(images, labels):\n",
    "    t = rng.draw(labels.shape[0])[:, 0].to(device)\n",
    "\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    alphas = alphas[:, None, None, None]\n",
    "    sigmas = sigmas[:, None, None, None]\n",
    "\n",
    "    noise = torch.randn_like(images)\n",
    "    noised_reals = images * alphas + noise * sigmas\n",
    "\n",
    "    targets = noise * alphas - images * sigmas\n",
    "\n",
    "    return t, noised_reals, targets\n",
    "\n",
    "def train_diffussion(epoch, model, train_dl, optimizer, scheduler):\n",
    "    model.train()\n",
    "    for src_images, src_labels in train_dl:\n",
    "            src_images, src_labels = src_images.to(device), src_labels.to(device)\n",
    "\n",
    "            t, noised_src, src_recon_targets = generate_diffussion_target(src_images, src_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            to_drop = torch.rand(src_labels.shape, device=src_labels.device).le(0.2)\n",
    "            classes_drop = torch.where(to_drop, -torch.ones_like(src_labels), src_labels)\n",
    "\n",
    "            output = model(noised_src, t, classes_drop)\n",
    "            diffused_loss = F.mse_loss(output, src_recon_targets)\n",
    "\n",
    "            loss = diffused_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print('diff', loss.item())\n",
    "    noise = torch.randn([10, 3, 32, 32], device=device)\n",
    "    fakes_classes = torch.arange(10, device=device)\n",
    "    fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
    "    fakes = (fakes + 1) / 2\n",
    "    fakes = torch.clamp(fakes, min=0, max = 1)\n",
    "    save_image(fakes.data, './output/%03d_train.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dl = load_data_set(batch_size = batch_size)\n",
    "criterion = nn.NLLLoss()\n",
    "model = Diffusion().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size = 1, gamma = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "active_domian_loss = 50\n",
    "src_domain_label = torch.zeros(batch_size).long().to(device)\n",
    "tgt_domain_label = torch.ones(batch_size).long().to(device)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        train_diffussion(epoch, model, train_dl, optimizer, scheduler)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/09_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/020_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/029_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/039_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output/049_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
