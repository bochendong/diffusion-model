{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import math\n",
    "from torch.autograd import Function\n",
    "from torchvision.models import resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"\n",
    "    Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\n",
    "    \"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "files = glob.glob(\"./output/*.png\")\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoches = 50\n",
    "ema_decay = 0.999\n",
    "steps = 500\n",
    "eta = 1.\n",
    "\n",
    "guidance_scale = 2.\n",
    "\n",
    "def load_data_set(batch_size=64):\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "\n",
    "    # Load the entire CIFAR10 dataset\n",
    "    full_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf)\n",
    "\n",
    "    # Create filtered Subsets for each category\n",
    "    source_indices = [i for i, (_, label) in enumerate(full_dataset) if label < 8]\n",
    "    target_indices = [i for i, (_, label) in enumerate(full_dataset) if label == 8]\n",
    "    test_indices = [i for i, (_, label) in enumerate(full_dataset) if label == 9]\n",
    "\n",
    "    source_set = Subset(full_dataset, source_indices)\n",
    "    target_set = Subset(full_dataset, target_indices)\n",
    "    test_set = Subset(full_dataset, test_indices)\n",
    "\n",
    "    # Create DataLoaders for each subset\n",
    "    source_dl = DataLoader(source_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    target_dl = DataLoader(target_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    test_dl = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    return source_dl, target_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"\n",
    "    Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\n",
    "    \"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input) + self.skip(input)\n",
    "\n",
    "class ResConvBlock(ResidualBlock):\n",
    "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
    "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        super().__init__([\n",
    "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
    "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
    "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
    "        ], skip)\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert c_in % n_head == 0\n",
    "        self.norm = nn.GroupNorm(1, c_in)\n",
    "        self.n_head = n_head\n",
    "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
    "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "        qkv = self.qkv_proj(self.norm(input))\n",
    "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = k.shape[3]**-0.25\n",
    "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
    "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
    "        return input + self.dropout(self.out_proj(y))\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Linear(1000, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 3072),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        self.pretrained_model = self.pretrained_model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.pretrained_model(x)\n",
    "        x = self.fe(x)\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        return x\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 256 * 8 * 8)\n",
    "        return self.net(input)\n",
    "\n",
    "def expand_to_planes(input, shape):\n",
    "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64\n",
    "        self.feature_embed = FeatureEmbedding()\n",
    "        self.domain_classifier = DomainClassifier()\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.class_embed = nn.Embedding(11, 4)\n",
    "\n",
    "        self.avg_1 = nn.AvgPool2d(2)\n",
    "        self.avg_2 = nn.AvgPool2d(2)\n",
    "        self.avg_3 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.down_0 = nn.Sequential(\n",
    "            ResConvBlock(3 + 16 + 4, c, c),\n",
    "            ResConvBlock(c, c, c),\n",
    "        )\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            ResConvBlock(c, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c * 2),\n",
    "        )\n",
    "\n",
    "        self.down_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "        self.down_3 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 8),\n",
    "            SelfAttention2d(c * 8, c * 8 // 64),\n",
    "            ResConvBlock(c * 8, c * 8, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "        )\n",
    "\n",
    "        self.up_0 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.up_1 = nn.Sequential(\n",
    "            ResConvBlock(c * 8, c * 4, c * 4),\n",
    "            SelfAttention2d(c * 4, c * 4 // 64),\n",
    "            ResConvBlock(c * 4, c * 4, c * 2),\n",
    "            SelfAttention2d(c * 2, c * 2 // 64),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_2 = nn.Sequential(\n",
    "            ResConvBlock(c * 4, c * 2, c * 2),\n",
    "            ResConvBlock(c * 2, c * 2, c),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "        )\n",
    "\n",
    "        self.up_3 = nn.Sequential(\n",
    "            ResConvBlock(c * 2, c, c),\n",
    "            ResConvBlock(c, c, 3, is_last=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t, cond, type = \"Source\"):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
    "\n",
    "        if (type == \"Target\"):\n",
    "            feature_embed = self.feature_embed(input)\n",
    "            x = torch.cat([feature_embed, timestep_embed, class_embed], dim=1)\n",
    "        else:\n",
    "            x = torch.cat([input, timestep_embed, class_embed], dim=1)\n",
    "\n",
    "        # Down Sample\n",
    "        down_sample = self.down_0(x)\n",
    "        identity_0 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_1(self.down_1(down_sample))\n",
    "        identity_1 = nn.Identity()(down_sample)\n",
    "\n",
    "        down_sample = self.avg_2(self.down_2(down_sample))\n",
    "        identity_2 = nn.Identity()(down_sample)\n",
    "\n",
    "        # Middle Sample\n",
    "        middle_sample_0 = self.down_3(down_sample)\n",
    "        middle_sample = self.avg_3(self.up_0(middle_sample_0))\n",
    "\n",
    "        reverse_feature = ReverseLayerF.apply(middle_sample_0, 0.005)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        # Up Sample\n",
    "        up_sample = torch.cat([middle_sample, identity_2], dim=1)\n",
    "        up_sample = self.up_1(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_1], dim=1)\n",
    "        up_sample = self.up_2(up_sample)\n",
    "\n",
    "        up_sample = torch.cat([up_sample, identity_0], dim=1)\n",
    "\n",
    "        return self.up_3(up_sample), domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
    "    \"\"\"\n",
    "    Draws samples from a model given starting noise.\n",
    "    \"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in range(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x_in = torch.cat([x, x])\n",
    "            ts_in = torch.cat([ts, ts])\n",
    "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
    "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in)[0].float().chunk(2)\n",
    "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "steps = 500\n",
    "eta = 1.\n",
    "ema_decay = 0.999\n",
    "guidance_scale = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffussion_target(images, labels):\n",
    "    t = rng.draw(labels.shape[0])[:, 0].to(device)\n",
    "\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    alphas = alphas[:, None, None, None]\n",
    "    sigmas = sigmas[:, None, None, None]\n",
    "\n",
    "    noise = torch.randn_like(images)\n",
    "    noised_reals = images * alphas + noise * sigmas\n",
    "\n",
    "    targets = noise * alphas - images * sigmas\n",
    "\n",
    "    return t, noised_reals, targets\n",
    "\n",
    "def train_model(model, optimizer, source_dl, target_dl, criterion, epoches, device, batch_size, scheduler):\n",
    "    src_domain_label = torch.zeros(batch_size).long().to(device)\n",
    "    tgt_domain_label = torch.ones(batch_size).long().to(device)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        total_src_loss = 0\n",
    "        total_tgt_loss = 0\n",
    "        total_diff_loss = 0\n",
    "        source_iter = iter(source_dl)\n",
    "        model.train()\n",
    "\n",
    "        for tgt_images, tgt_labels in target_dl:\n",
    "            tgt_images, tgt_labels = tgt_images.to(device), tgt_labels.to(device)\n",
    "            src_images, src_labels = next(source_iter)\n",
    "            src_images, src_labels = src_images.to(device), src_labels.to(device)\n",
    "\n",
    "            t, noised_src, src_recon_targets = generate_diffussion_target(src_images, src_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            to_drop = torch.rand(src_labels.shape, device=src_labels.device).le(0.2)\n",
    "            classes_drop = torch.where(to_drop, -torch.ones_like(src_labels), src_labels)\n",
    "\n",
    "            output, domain_output = model(noised_src, t, classes_drop, type = \"Source\")\n",
    "            diffused_loss = F.mse_loss(output, src_recon_targets)\n",
    "            loss_s_domain = criterion(domain_output, src_domain_label)\n",
    "\n",
    "            output, domain_output = model(tgt_images, t, tgt_labels, type = \"Target\")\n",
    "            loss_t_domain = criterion(domain_output, tgt_domain_label)\n",
    "\n",
    "            loss = (loss_s_domain + loss_t_domain) / 6. + diffused_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_src_loss += loss_s_domain\n",
    "            total_tgt_loss += loss_t_domain\n",
    "            total_diff_loss += diffused_loss\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(\"src\", total_src_loss.item()/len(target_dl), 'tgt', total_tgt_loss.item()/len(target_dl), 'diff', total_diff_loss.item()/len(target_dl))\n",
    "        noise = torch.randn([10, 3, 32, 32], device=device)\n",
    "        fakes_classes = torch.arange(10, device=device)\n",
    "        fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
    "        fakes = (fakes + 1) / 2\n",
    "        fakes = torch.clamp(fakes, min=0, max = 1)\n",
    "        save_image(fakes.data, './output/%03d_train.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "source_dl, target_dl, test_dl = load_data_set(batch_size = batch_size)\n",
    "criterion = nn.NLLLoss()\n",
    "model = Diffusion().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len source 312\n",
      "len target 39\n",
      "len target 39\n"
     ]
    }
   ],
   "source": [
    "print(\"len source\", len(source_dl))\n",
    "print(\"len target\", len(target_dl))\n",
    "print(\"len target\", len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "src 0.7340480119754107 tgt 0.6951500819279597 diff 0.46077747834034455\n",
      "Epoch 2:\n",
      "src 0.6683713961870242 tgt 0.668187752748147 diff 0.23487245119535005\n",
      "Epoch 3:\n",
      "src 0.6470091893122747 tgt 0.6473362751496143 diff 0.2015992678128756\n",
      "Epoch 4:\n",
      "src 0.6398609357002454 tgt 0.642402110955654 diff 0.19111707883003431\n",
      "Epoch 5:\n",
      "src 0.5954394707312951 tgt 0.6146062704233023 diff 0.18351255319057366\n",
      "Epoch 6:\n",
      "src 0.5260889102251102 tgt 0.5209430303329077 diff 0.17772824947650617\n",
      "Epoch 7:\n",
      "src 0.5639374561798878 tgt 0.5734321398612781 diff 0.17131683153983873\n",
      "Epoch 8:\n",
      "src 0.6197876563439002 tgt 0.644503177740635 diff 0.1699825311318422\n",
      "Epoch 9:\n",
      "src 0.6278642018636068 tgt 0.6373319870386368 diff 0.1663051140614045\n",
      "Epoch 10:\n",
      "src 0.6373699872921674 tgt 0.6478815323267227 diff 0.16147186817267004\n",
      "Epoch 11:\n",
      "src 0.6531507540971805 tgt 0.6545757391513922 diff 0.15857864037538186\n",
      "Epoch 12:\n",
      "src 0.6114017780010517 tgt 0.618862298818735 diff 0.15706967084835738\n",
      "Epoch 13:\n",
      "src 0.5839115045009515 tgt 0.6073524768535907 diff 0.15542820172432142\n",
      "Epoch 14:\n",
      "src 0.6114026583158053 tgt 0.6023306724352714 diff 0.15426023189838117\n",
      "Epoch 15:\n",
      "src 0.6430859688000802 tgt 0.6233694614508213 diff 0.15240719379522863\n",
      "Epoch 16:\n",
      "src 0.6178798186473358 tgt 0.6122536781506661 diff 0.1524185156210875\n",
      "Epoch 17:\n",
      "src 0.5851950523180839 tgt 0.5797899686373197 diff 0.15026164666200295\n",
      "Epoch 18:\n",
      "src 0.6170220008263221 tgt 0.623904057038136 diff 0.15062125523885092\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, source_dl, target_dl, criterion = criterion,\n",
    "            epoches=epoches, device=device, batch_size = batch_size, scheduler = scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
