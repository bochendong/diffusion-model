{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9cHO83sjuHBIk7zZ2xEPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a961fe011004d5aab55e5635f2759d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f92212b4deb46a4badc2b0573fff788",
              "IPY_MODEL_a0dfeef81ae64148a884c11d6af7bff3",
              "IPY_MODEL_ff3138a2a1be4ba5a2ae6c8a5819f192"
            ],
            "layout": "IPY_MODEL_55e4349acdf94732bf162e5392c4f209"
          }
        },
        "9f92212b4deb46a4badc2b0573fff788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25fc051271d74327b240f76e84280e43",
            "placeholder": "​",
            "style": "IPY_MODEL_4b69c33f63624324a73056842a625123",
            "value": " 22%"
          }
        },
        "a0dfeef81ae64148a884c11d6af7bff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d223e552114e39a507ef6d7e405373",
            "max": 781,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_382a5b4033f54118b2b3f4d358224e63",
            "value": 174
          }
        },
        "ff3138a2a1be4ba5a2ae6c8a5819f192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48802d2c1ae04c1db66cf89f93509ff3",
            "placeholder": "​",
            "style": "IPY_MODEL_cf43bdbf2ac547f69f70f8b176318ab0",
            "value": " 174/781 [01:31&lt;05:04,  1.99it/s]"
          }
        },
        "55e4349acdf94732bf162e5392c4f209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25fc051271d74327b240f76e84280e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b69c33f63624324a73056842a625123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d223e552114e39a507ef6d7e405373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382a5b4033f54118b2b3f4d358224e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48802d2c1ae04c1db66cf89f93509ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf43bdbf2ac547f69f70f8b176318ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/diffusion-model/blob/main/DDPM/DDPM_DANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1lu-xkj_DbM"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Function\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "9q4lPoJf0-ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fcd1a3-b5cc-42ca-af3b-57d15341b96b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_alphas_sigmas(t):\n",
        "    \"\"\"\n",
        "    Returns the scaling factors for the clean image (alpha) and for the\n",
        "    noise (sigma), given a timestep.\n",
        "    \"\"\"\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "files = glob.glob(\"./output/*.png\")\n",
        "\n",
        "for f in files:\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "bLGurZOw_KRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Set"
      ],
      "metadata": {
        "id": "SwW2cWHzugrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epoches = 30\n",
        "ema_decay = 0.999\n",
        "steps = 500\n",
        "eta = 1.\n",
        "\n",
        "guidance_scale = 2.\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "MuyTXC4x_O0C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_set = datasets.CIFAR10(root='data', train=True, download=True, transform=tf, target_transform=lambda x: x if x < 5 else -1)\n",
        "source_dl = DataLoader(source_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last = True)\n",
        "\n",
        "target_set = datasets.CIFAR10(root='data', train=True, download=True, transform=tf, target_transform=lambda x: x if (x <= 7 and x >= 5) else -1)\n",
        "target_dl = DataLoader(target_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last = True)\n",
        "\n",
        "test_set = datasets.CIFAR10(root='data', train=True, download=True, transform=tf, target_transform=lambda x: x if (x >= 8) else -1)\n",
        "test_dl = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last = True)"
      ],
      "metadata": {
        "id": "W8b12jQ8CkI_",
        "outputId": "f2813afe-33b9-41b6-9327-35c718c3288e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "QUGf43CtukEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidualBlock"
      ],
      "metadata": {
        "id": "3nK_CfVerRt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input) + self.skip(input)\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
        "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
        "        ], skip)"
      ],
      "metadata": {
        "id": "0KWdK_SjrOP5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention"
      ],
      "metadata": {
        "id": "kX9ZFm-DrUUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention2d(nn.Module):\n",
        "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        assert c_in % n_head == 0\n",
        "        self.norm = nn.GroupNorm(1, c_in)\n",
        "        self.n_head = n_head\n",
        "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
        "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        qkv = self.qkv_proj(self.norm(input))\n",
        "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = k.shape[3]**-0.25\n",
        "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
        "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
        "        return input + self.dropout(self.out_proj(y))"
      ],
      "metadata": {
        "id": "-nyoos5frW1t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FourierFeatures"
      ],
      "metadata": {
        "id": "Desn3yZmrZkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)"
      ],
      "metadata": {
        "id": "DNACvcAira-Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Embedding"
      ],
      "metadata": {
        "id": "pjZi3AH697XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureEmbedding, self).__init__()\n",
        "        self.fc1 = nn.Linear(1000, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 3072)\n",
        "\n",
        "        self.pretrained_model = resnet50(pretrained=True)\n",
        "        self.pretrained_model = self.pretrained_model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.pretrained_model(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = x.view(-1, 3, 32, 32)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-UCra0_2997T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Classifier"
      ],
      "metadata": {
        "id": "ZWFtgpU3Af8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(256 * 8 * 8, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 2),\n",
        "            nn.LogSoftmax(dim = 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.view(-1, 256 * 8 * 8)\n",
        "        return self.net(input)"
      ],
      "metadata": {
        "id": "SDfyKrO6AgDN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unet"
      ],
      "metadata": {
        "id": "ShXgb4RVrdxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])"
      ],
      "metadata": {
        "id": "e-T-HkLCrdUx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64\n",
        "        self.feature_embed = FeatureEmbedding()\n",
        "        self.domain_classifier = DomainClassifier()\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.class_embed = nn.Embedding(11, 4)\n",
        "\n",
        "        self.avg_1 = nn.AvgPool2d(2)\n",
        "        self.avg_2 = nn.AvgPool2d(2)\n",
        "        self.avg_3 = nn.AvgPool2d(2)\n",
        "\n",
        "        self.down_0 = nn.Sequential(\n",
        "            ResConvBlock(3 + 16 + 4, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "        )\n",
        "\n",
        "        self.down_1 = nn.Sequential(\n",
        "            ResConvBlock(c, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "        )\n",
        "\n",
        "        self.down_2 = nn.Sequential(\n",
        "            ResConvBlock(c * 2, c * 4, c * 4),\n",
        "            SelfAttention2d(c * 4, c * 4 // 64),\n",
        "            ResConvBlock(c * 4, c * 4, c * 4),\n",
        "            SelfAttention2d(c * 4, c * 4 // 64),\n",
        "        )\n",
        "\n",
        "        self.down_3 = nn.Sequential(\n",
        "            ResConvBlock(c * 4, c * 8, c * 8),\n",
        "            SelfAttention2d(c * 8, c * 8 // 64),\n",
        "            ResConvBlock(c * 8, c * 8, c * 8),\n",
        "            SelfAttention2d(c * 8, c * 8 // 64),\n",
        "            ResConvBlock(c * 8, c * 8, c * 8),\n",
        "            SelfAttention2d(c * 8, c * 8 // 64),\n",
        "            ResConvBlock(c * 8, c * 8, c * 4),\n",
        "            SelfAttention2d(c * 4, c * 4 // 64),\n",
        "        )\n",
        "\n",
        "        self.up_0 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.up_1 = nn.Sequential(\n",
        "            ResConvBlock(c * 8, c * 4, c * 4),\n",
        "            SelfAttention2d(c * 4, c * 4 // 64),\n",
        "            ResConvBlock(c * 4, c * 4, c * 2),\n",
        "            SelfAttention2d(c * 2, c * 2 // 64),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "        )\n",
        "\n",
        "        self.up_2 = nn.Sequential(\n",
        "            ResConvBlock(c * 4, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "        )\n",
        "\n",
        "        self.up_3 = nn.Sequential(\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, is_last=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t, cond, type = \"Target\"):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
        "\n",
        "        if (type == \"Target\"):\n",
        "            feature_embed = self.feature_embed(input)\n",
        "            x = torch.cat([feature_embed, timestep_embed, class_embed], dim=1)\n",
        "        else:\n",
        "            x = torch.cat([input, timestep_embed, class_embed], dim=1)\n",
        "\n",
        "        # Down Sample\n",
        "        down_sample = self.down_0(x)\n",
        "        identity_0 = nn.Identity()(down_sample)\n",
        "\n",
        "        down_sample = self.avg_1(self.down_1(down_sample))\n",
        "        identity_1 = nn.Identity()(down_sample)\n",
        "\n",
        "        down_sample = self.avg_2(self.down_2(down_sample))\n",
        "        identity_2 = nn.Identity()(down_sample)\n",
        "\n",
        "        # Middle Sample\n",
        "        middle_sample = self.down_3(down_sample)\n",
        "        middle_sample = self.avg_3(self.up_0(middle_sample))\n",
        "\n",
        "        reverse_feature = ReverseLayerF.apply(middle_sample, 0.01)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "        # Up Sample\n",
        "        up_sample = torch.cat([middle_sample, identity_2], dim=1)\n",
        "        up_sample = self.up_1(up_sample)\n",
        "\n",
        "        up_sample = torch.cat([up_sample, identity_1], dim=1)\n",
        "        up_sample = self.up_2(up_sample)\n",
        "\n",
        "        up_sample = torch.cat([up_sample, identity_0], dim=1)\n",
        "\n",
        "        return self.up_3(up_sample), domain_output"
      ],
      "metadata": {
        "id": "oHGW6xPXONvN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recover Image"
      ],
      "metadata": {
        "id": "5wcOuoCFrpfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
        "    \"\"\"\n",
        "    Draws samples from a model given starting noise.\n",
        "    \"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
        "    alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(steps):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x_in = torch.cat([x, x])\n",
        "            ts_in = torch.cat([ts, ts])\n",
        "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
        "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in)[0].float().chunk(2)\n",
        "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < steps - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6C4vOiJCrnpA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Stage"
      ],
      "metadata": {
        "id": "NvfNyDUxrtVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "UphLta0rrw07"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_diffussion_target(images, labels):\n",
        "    t = rng.draw(labels.shape[0])[:, 0].to(device)\n",
        "\n",
        "    alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "    alphas = alphas[:, None, None, None]\n",
        "    sigmas = sigmas[:, None, None, None]\n",
        "\n",
        "    noise = torch.randn_like(images)\n",
        "    noised_reals = images * alphas + noise * sigmas\n",
        "\n",
        "    targets = noise * alphas - images * sigmas\n",
        "\n",
        "    return t, noised_reals, targets\n",
        "\n",
        "def train_model(model, optimizer, source_dl, target_dl, epochs, device):\n",
        "    src_domain_label = torch.zeros(batch_size).long().to(device)\n",
        "    tgt_domain_label = torch.ones(batch_size).long().to(device)\n",
        "\n",
        "    total_src_loss = 0\n",
        "    total_tgt_loss = 0\n",
        "    total_diff_loss = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        target_iter = iter(target_dl)\n",
        "        model.train()\n",
        "        for src_images, src_labels in tqdm(source_dl):\n",
        "            # Source\n",
        "            src_images, src_labels = src_images.to(device), src_labels.to(device)\n",
        "\n",
        "            t, noised_src, src_recon_targets = generate_diffussion_target(src_images, src_labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output, domain_output = model(noised_src, t, src_labels, type = \"Source\")\n",
        "            diffused_loss = F.mse_loss(output, src_recon_targets)\n",
        "\n",
        "            loss_s_domain = criterion(domain_output, src_domain_label)\n",
        "\n",
        "            # Target\n",
        "            tgt_images, tgt_labels = next(target_iter)\n",
        "            tgt_images, tgt_labels = tgt_images.to(device), tgt_labels.to(device)\n",
        "\n",
        "            output, domain_output = model(tgt_images, t, tgt_labels, type = \"Target\")\n",
        "\n",
        "            loss_t_domain = criterion(domain_output, tgt_domain_label)\n",
        "\n",
        "            # TODO: It should be a class recon classifier here\n",
        "\n",
        "            loss = (diffused_loss + loss_s_domain +  loss_t_domain)\n",
        "\n",
        "            total_src_loss += loss_s_domain\n",
        "            total_tgt_loss += loss_t_domain\n",
        "            total_diff_loss = diffused_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"src\", total_src_loss, 'tgt', total_tgt_loss, 'diff', total_diff_loss)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
        "        noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "        fakes_classes = torch.arange(10, device=device)\n",
        "        fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "        fakes = (fakes + 1) / 2\n",
        "        fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "        save_image(fakes.data, './output/%03d_train.png' % epoch)"
      ],
      "metadata": {
        "id": "uXMRwa5Te6NZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "0Q4oRM44sMut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Diffusion().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-uWO64r4-3",
        "outputId": "0cdc7db6-7ca9-47c8-f81a-a1116797400b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, optimizer, source_dl, target_dl, epochs=30, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "9a961fe011004d5aab55e5635f2759d8",
            "9f92212b4deb46a4badc2b0573fff788",
            "a0dfeef81ae64148a884c11d6af7bff3",
            "ff3138a2a1be4ba5a2ae6c8a5819f192",
            "55e4349acdf94732bf162e5392c4f209",
            "25fc051271d74327b240f76e84280e43",
            "4b69c33f63624324a73056842a625123",
            "95d223e552114e39a507ef6d7e405373",
            "382a5b4033f54118b2b3f4d358224e63",
            "48802d2c1ae04c1db66cf89f93509ff3",
            "cf43bdbf2ac547f69f70f8b176318ab0"
          ]
        },
        "id": "YBWEb1Z6r9CG",
        "outputId": "6103754a-8821-447c-a368-015d7552ea28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/781 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a961fe011004d5aab55e5635f2759d8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/009_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7C6pQpzsA8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/019_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tnq0YvUasFAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./output/029_train.png\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-bczJEuQsFGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Stage"
      ],
      "metadata": {
        "id": "uIwAe5GJupC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''def train_with_features(model, optimizer, train_dl, feature_dl, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, ((reals, classes), (features, feature_labels)) in enumerate(zip(train_dl, feature_dl)):\n",
        "            optimizer.zero_grad()\n",
        "            reals, classes = reals.to(device), classes.to(device)\n",
        "            features = features.to(device)\n",
        "\n",
        "            t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "            alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "            alphas = alphas[:, None, None, None]\n",
        "            sigmas = sigmas[:, None, None, None]\n",
        "\n",
        "            noise = torch.randn_like(reals)\n",
        "            noised_reals = reals * alphas + noise * sigmas\n",
        "            targets = noise * alphas - reals * sigmas\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                v = model(noised_reals, t, classes, external_features=features)\n",
        "                loss = F.mse_loss(v, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "        fakes_classes = torch.arange(10, device=device)\n",
        "\n",
        "        fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "        fakes = (fakes + 1) / 2\n",
        "        fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "        save_image(fakes.data, './output/%03d_imagine.png' % epoch)'''"
      ],
      "metadata": {
        "id": "ifO4jcg7e8WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Train the model with features\n",
        "train_with_features(model, optimizer, train_dl, feature_dl, epochs=40, device=device)'''"
      ],
      "metadata": {
        "id": "O9nLL3abkdTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def train(epoch):\n",
        "    for i, (reals, classes) in enumerate(tqdm(train_dl)):\n",
        "        opt.zero_grad()\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        # Draw uniformly distributed continuous timesteps\n",
        "        t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "\n",
        "        # Calculate the noise schedule parameters for those timesteps\n",
        "        alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "        # Combine the ground truth images and the noise\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        noise = torch.randn_like(reals)\n",
        "\n",
        "        # with t increase, alpha goes down and sigmas goes up\n",
        "        # which means with t increase, noised_reals will have more noise and less image\n",
        "        noised_reals = reals * alphas + noise * sigmas\n",
        "\n",
        "        # with t increase, alpha goes down and sigmas goes up\n",
        "        # which means with t increase, targets will closer to - img\n",
        "        targets = noise * alphas - reals * sigmas\n",
        "\n",
        "        # Drop out the class on 20% of the examples\n",
        "        to_drop = torch.rand(classes.shape, device=classes.device).le(0.2)\n",
        "        classes_drop = torch.where(to_drop, -torch.ones_like(classes), classes)\n",
        "\n",
        "        # Compute the model output and the loss.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            v = model(noised_reals, t, classes_drop)\n",
        "            loss = F.mse_loss(v, targets)\n",
        "\n",
        "        # Do the optimizer step and EMA update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "\n",
        "\n",
        "        # ema_update(model, model_ema, 0.95 if epoch < 20 else ema_decay)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            tqdm.write(f'Epoch: {epoch}, iteration: {i}, loss: {loss.item():g}')\n",
        "\n",
        "    noise = torch.randn([10, 3, 32, 32], device=device)\n",
        "    fakes_classes = torch.arange(10, device=device)\n",
        "\n",
        "    fakes = sample(model, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "    fakes = (fakes + 1) / 2\n",
        "    fakes = torch.clamp(fakes, min=0, max = 1)\n",
        "    save_image(fakes.data, './output/%03d.png' % epoch)\n",
        "'''"
      ],
      "metadata": {
        "id": "5Hl1eaHr_d4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''try:\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        train(i)\n",
        "except KeyboardInterrupt:\n",
        "    pass'''"
      ],
      "metadata": {
        "id": "uV9DM-JvAFYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}