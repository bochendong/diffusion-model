{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/diffusion-model/blob/main/CIFAR_10_Diffusion_(version_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0w6Isp26ay"
      },
      "source": [
        "# Trains a diffusion model on CIFAR-10 (version 4).\n",
        "\n",
        "By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings).\n",
        "\n",
        "The model is a denoising diffusion probabilistic model (https://arxiv.org/abs/2006.11239), which is trained to reverse a gradual noising process, allowing the model to generate samples from the learned data distribution starting from random noise. DDIM-style deterministic sampling (https://arxiv.org/abs/2010.02502) is also supported. This model is also trained on continuous timesteps. It uses the 'v' objective from Progressive Distillation for Fast Sampling of Diffusion Models (https://openreview.net/forum?id=TIdIXIpzhoI). It is usable for classifier-free diffusion guidance (https://openreview.net/forum?id=qw8AKxfYbI)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUwPLG92r89",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M99bmqIPyw_Q"
      },
      "source": [
        "# Check the GPU type\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w5A9GHfynNT"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm, trange\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8IFYM8fy5h8"
      },
      "source": [
        "# Utilities\n",
        "\n",
        "@contextmanager\n",
        "def train_mode(model, mode=True):\n",
        "    \"\"\"A context manager that places a model into training mode and restores\n",
        "    the previous mode on exit.\"\"\"\n",
        "    modes = [module.training for module in model.modules()]\n",
        "    try:\n",
        "        yield model.train(mode)\n",
        "    finally:\n",
        "        for i, module in enumerate(model.modules()):\n",
        "            module.training = modes[i]\n",
        "\n",
        "\n",
        "def eval_mode(model):\n",
        "    \"\"\"A context manager that places a model into evaluation mode and restores\n",
        "    the previous mode on exit.\"\"\"\n",
        "    return train_mode(model, False)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ema_update(model, averaged_model, decay):\n",
        "    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n",
        "    version of a model. It should be called after each optimizer step.\"\"\"\n",
        "    model_params = dict(model.named_parameters())\n",
        "    averaged_params = dict(averaged_model.named_parameters())\n",
        "    assert model_params.keys() == averaged_params.keys()\n",
        "\n",
        "    for name, param in model_params.items():\n",
        "        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n",
        "\n",
        "    model_buffers = dict(model.named_buffers())\n",
        "    averaged_buffers = dict(averaged_model.named_buffers())\n",
        "    assert model_buffers.keys() == averaged_buffers.keys()\n",
        "\n",
        "    for name, buf in model_buffers.items():\n",
        "        averaged_buffers[name].copy_(buf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DR14Jaly8FZ"
      },
      "source": [
        "# Define the model (a residual U-Net)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input) + self.skip(input)\n",
        "\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, is_last=False):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True) if not is_last else nn.Identity(),\n",
        "            nn.ReLU(inplace=True) if not is_last else nn.Identity(),\n",
        "        ], skip)\n",
        "\n",
        "\n",
        "class SelfAttention2d(nn.Module):\n",
        "    def __init__(self, c_in, n_head=1, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        assert c_in % n_head == 0\n",
        "        self.norm = nn.GroupNorm(1, c_in)\n",
        "        self.n_head = n_head\n",
        "        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)\n",
        "        self.out_proj = nn.Conv2d(c_in, c_in, 1)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        qkv = self.qkv_proj(self.norm(input))\n",
        "        qkv = qkv.view([n, self.n_head * 3, c // self.n_head, h * w]).transpose(2, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = k.shape[3]**-0.25\n",
        "        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)\n",
        "        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])\n",
        "        return input + self.dropout(self.out_proj(y))\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "\n",
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.class_embed = nn.Embedding(11, 4)\n",
        "\n",
        "        self.net = nn.Sequential(   # 32x32\n",
        "            ResConvBlock(3 + 16 + 4, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            SkipBlock([\n",
        "                nn.AvgPool2d(2),  # 32x32 -> 16x16\n",
        "                ResConvBlock(c, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.AvgPool2d(2),  # 16x16 -> 8x8\n",
        "                    ResConvBlock(c * 2, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    SkipBlock([\n",
        "                        nn.AvgPool2d(2),  # 8x8 -> 4x4\n",
        "                        ResConvBlock(c * 4, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        SelfAttention2d(c * 8, c * 8 // 64),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 4),\n",
        "                        SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                    ]),  # 4x4 -> 8x8\n",
        "                    ResConvBlock(c * 8, c * 4, c * 4),\n",
        "                    SelfAttention2d(c * 4, c * 4 // 64),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 2),\n",
        "                    SelfAttention2d(c * 2, c * 2 // 64),\n",
        "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                ]),  # 8x8 -> 16x16\n",
        "                ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ]),  # 16x16 -> 32x32\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, is_last=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t, cond):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cond + 1), input.shape)\n",
        "        return self.net(torch.cat([input, class_embed, timestep_embed], dim=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpy3GC7XzC7J"
      },
      "source": [
        "# Define the noise schedule and sampling loop\n",
        "\n",
        "def get_alphas_sigmas(t):\n",
        "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
        "    noise (sigma), given a timestep.\"\"\"\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, eta, classes, guidance_scale=1.):\n",
        "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
        "    alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(steps):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x_in = torch.cat([x, x])\n",
        "            ts_in = torch.cat([ts, ts])\n",
        "            classes_in = torch.cat([-torch.ones_like(classes), classes])\n",
        "            v_uncond, v_cond = model(x_in, ts_in * t[i], classes_in).float().chunk(2)\n",
        "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < steps - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYdls2GREKLv"
      },
      "source": [
        "# Visualize the noise schedule\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "t_vis = torch.linspace(0, 1, 1000)\n",
        "alphas_vis, sigmas_vis = get_alphas_sigmas(t_vis)\n",
        "\n",
        "print('The noise schedule:')\n",
        "\n",
        "plt.plot(t_vis, alphas_vis, label='alpha (signal level)')\n",
        "plt.plot(t_vis, sigmas_vis, label='sigma (noise level)')\n",
        "plt.legend()\n",
        "plt.xlabel('timestep')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMWkwqn6zSZ5"
      },
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "train_set = datasets.CIFAR10('data', train=True, download=True, transform=tf)\n",
        "train_dl = data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                           num_workers=4, persistent_workers=True, pin_memory=True)\n",
        "val_set = datasets.CIFAR10('data', train=False, download=True, transform=tf)\n",
        "val_dl = data.DataLoader(val_set, batch_size,\n",
        "                         num_workers=4, persistent_workers=True, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egOFiEQ_zL25"
      },
      "source": [
        "# Create the model and optimizer\n",
        "\n",
        "seed = 0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = Diffusion().to(device)\n",
        "model_ema = deepcopy(model)\n",
        "print('Model parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "epoch = 0\n",
        "\n",
        "# Use a low discrepancy quasi-random sequence to sample uniformly distributed\n",
        "# timesteps. This considerably reduces the between-batch variance of the loss.\n",
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNYA6yzzuXY"
      },
      "source": [
        "# Actually train the model\n",
        "\n",
        "ema_decay = 0.999\n",
        "\n",
        "# The number of timesteps to use when sampling\n",
        "steps = 500\n",
        "\n",
        "# The amount of noise to add each timestep when sampling\n",
        "# 0 = no noise (DDIM)\n",
        "# 1 = full noise (DDPM)\n",
        "eta = 1.\n",
        "\n",
        "# Classifier-free guidance scale (0 is unconditional, 1 is conditional)\n",
        "guidance_scale = 2.\n",
        "\n",
        "def eval_loss(model, rng, reals, classes):\n",
        "    # Draw uniformly distributed continuous timesteps\n",
        "    t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "\n",
        "    # Calculate the noise schedule parameters for those timesteps\n",
        "    alphas, sigmas = get_alphas_sigmas(t)\n",
        "\n",
        "    # Combine the ground truth images and the noise\n",
        "    alphas = alphas[:, None, None, None]\n",
        "    sigmas = sigmas[:, None, None, None]\n",
        "    noise = torch.randn_like(reals)\n",
        "    noised_reals = reals * alphas + noise * sigmas\n",
        "    targets = noise * alphas - reals * sigmas\n",
        "\n",
        "    # Drop out the class on 20% of the examples\n",
        "    to_drop = torch.rand(classes.shape, device=classes.device).le(0.2)\n",
        "    classes_drop = torch.where(to_drop, -torch.ones_like(classes), classes)\n",
        "\n",
        "    # Compute the model output and the loss.\n",
        "    with torch.cuda.amp.autocast():\n",
        "        v = model(noised_reals, t, classes_drop)\n",
        "        return F.mse_loss(v, targets)\n",
        "\n",
        "\n",
        "def train():\n",
        "    for i, (reals, classes) in enumerate(tqdm(train_dl)):\n",
        "        opt.zero_grad()\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        # Evaluate the loss\n",
        "        loss = eval_loss(model, rng, reals, classes)\n",
        "\n",
        "        # Do the optimizer step and EMA update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        ema_update(model, model_ema, 0.95 if epoch < 20 else ema_decay)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            tqdm.write(f'Epoch: {epoch}, iteration: {i}, loss: {loss.item():g}')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "@torch.random.fork_rng()\n",
        "@eval_mode(model_ema)\n",
        "def val():\n",
        "    tqdm.write('\\nValidating...')\n",
        "    torch.manual_seed(seed)\n",
        "    rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    for i, (reals, classes) in enumerate(tqdm(val_dl)):\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        loss = eval_loss(model_ema, rng, reals, classes)\n",
        "\n",
        "        total_loss += loss.item() * len(reals)\n",
        "        count += len(reals)\n",
        "    loss = total_loss / count\n",
        "    tqdm.write(f'Validation: Epoch: {epoch}, loss: {loss:g}')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "@torch.random.fork_rng()\n",
        "@eval_mode(model_ema)\n",
        "def demo():\n",
        "    tqdm.write('\\nSampling...')\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    noise = torch.randn([100, 3, 32, 32], device=device)\n",
        "    fakes_classes = torch.arange(10, device=device).repeat_interleave(10, 0)\n",
        "    fakes = sample(model_ema, noise, steps, eta, fakes_classes, guidance_scale)\n",
        "\n",
        "    grid = utils.make_grid(fakes, 10).cpu()\n",
        "    filename = f'demo_{epoch:05}.png'\n",
        "    TF.to_pil_image(grid.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "    display.display(display.Image(filename))\n",
        "    tqdm.write('')\n",
        "\n",
        "\n",
        "def save():\n",
        "    filename = 'cifar_diffusion.pth'\n",
        "    obj = {\n",
        "        'model': model.state_dict(),\n",
        "        'model_ema': model_ema.state_dict(),\n",
        "        'opt': opt.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(obj, filename)\n",
        "\n",
        "\n",
        "try:\n",
        "    val()\n",
        "    demo()\n",
        "    while True:\n",
        "        print('Epoch', epoch)\n",
        "        train()\n",
        "        epoch += 1\n",
        "        if epoch % 5 == 0:\n",
        "            val()\n",
        "            demo()\n",
        "        save()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
